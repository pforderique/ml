{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc70a1be",
   "metadata": {},
   "source": [
    "# Shape-Net\n",
    "\n",
    "A simple neural network that can distinguish shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3e53ac",
   "metadata": {},
   "source": [
    "## Binary Shape Net\n",
    "First, we create a simple nueral net that can distinguish L-shapes from non L-shapes in 3x3 grayscale images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f85963e",
   "metadata": {},
   "source": [
    "### ðŸ“ Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0c3d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define dataset\n",
    "L_shapes = [\n",
    "    torch.tensor([[1,0,0],\n",
    "                  [1,0,0],\n",
    "                  [1,1,0]]),  # L\n",
    "    torch.tensor([[1,1,1],\n",
    "                  [1,0,0],\n",
    "                  [0,0,0]]),  # rotated L\n",
    "    torch.tensor([[0,0,1],\n",
    "                  [0,0,1],\n",
    "                  [0,1,1]]),  # another rotation\n",
    "    torch.tensor([[0,0,0],\n",
    "                  [0,0,1],\n",
    "                  [1,1,1]]),  # another rotation\n",
    "]\n",
    "\n",
    "non_L_shapes = [\n",
    "    torch.tensor([[1,1,0],\n",
    "                  [1,1,0],\n",
    "                  [0,0,0]]),  # square\n",
    "    torch.tensor([[1,0,0],\n",
    "                  [1,1,0],\n",
    "                  [0,0,0]]),  # corner\n",
    "    torch.tensor([[1,1,1],\n",
    "                  [0,1,0],\n",
    "                  [0,0,0]]),  # T-shape\n",
    "    torch.tensor([[0,1,0],\n",
    "                  [0,1,0],\n",
    "                  [0,1,0]]),  # column\n",
    "    torch.tensor([[1,1,1],\n",
    "                  [0,0,0],\n",
    "                  [0,0,0]]),  # sideways column\n",
    "    torch.tensor([[0,0,1],\n",
    "                  [0,0,1],\n",
    "                  [0,0,1]]),  # column\n",
    "    torch.tensor([[1,0,0],\n",
    "                  [1,1,0],\n",
    "                  [1,0,0]]),  # T\n",
    "    torch.tensor([[1,0,0],\n",
    "                  [0,0,0],\n",
    "                  [1,1,0]]),  # Near-L\n",
    "]\n",
    "\n",
    "X = torch.stack([x.float() for x in L_shapes + non_L_shapes]).unsqueeze(1)  # Add channel dimension (1 for grayscale) -> (B, 1, H, W)\n",
    "y = torch.tensor([1]*len(L_shapes) + [0]*len(non_L_shapes)).float().unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b9a87d",
   "metadata": {},
   "source": [
    "### ðŸ§  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88bf829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class ShapeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ShapeNet, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=2)  # 4 filters of size 2x2. One input channel (grayscale)\n",
    "        self.fc = nn.Linear(4 * 2 * 2, 1)  # Flattened 4 channels Ã— 2Ã—2 patch  1 output\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        x = torch.relu(self.conv(x))  # (B, 4, 2, 2)\n",
    "        x = x.view(x.size(0), -1)  # Flatten to (B, 16)\n",
    "        x = torch.sigmoid(self.fc(x))\n",
    "        return x\n",
    "\n",
    "    # This is only done to force intelliSense to recognize the return type as torch.Tensor\n",
    "    def __call__(self, *args: Any, **kwds: Any) -> torch.Tensor:\n",
    "        x = super().__call__(*args, **kwds)\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return x\n",
    "        raise TypeError(f\"Expected torch.Tensor, got {type(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606da2b9",
   "metadata": {},
   "source": [
    "### âš™ï¸ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d492dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ShapeNet()\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    y_pred = model(X)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Final predictions:\", y_pred.detach().round().squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4f7ae5",
   "metadata": {},
   "source": [
    "### ðŸ§ª Create Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bf5cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L-shaped (positive class)\n",
    "test_L = [\n",
    "    torch.tensor([[1,0,0],\n",
    "                  [1,0,0],\n",
    "                  [1,1,0]]),  # L\n",
    "    torch.tensor([[0,0,0],\n",
    "                  [0,0,1],\n",
    "                  [1,1,1]]),  # rotated L\n",
    "]\n",
    "\n",
    "# Non-L (negative class)\n",
    "test_non_L = [\n",
    "    torch.tensor([[1,1,0],\n",
    "                  [0,1,1],\n",
    "                  [0,0,0]]),  # diagonal blob\n",
    "    torch.tensor([[1,0,0],\n",
    "                  [1,0,0],\n",
    "                  [1,0,0]]),  # column\n",
    "]\n",
    "\n",
    "# Format test set\n",
    "X_test = torch.stack([x.float() for x in test_L + test_non_L]).unsqueeze(1)  # Add channel dimension (1 for grayscale) -> (B, 1, H, W)\n",
    "y_test = torch.tensor([1]*len(test_L) + [0]*len(test_non_L)).float().unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddc1101",
   "metadata": {},
   "source": [
    "### âœ… Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f27923",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # Disable gradient tracking for inference\n",
    "    y_pred = model(X_test)\n",
    "    print(\"Test predictions (raw):\", y_pred.squeeze())\n",
    "    y_pred_labels = (y_pred >= 0.5).float()  # Threshold at 0.5\n",
    "    print(\"Test predictions:\", y_pred_labels.squeeze())\n",
    "    correct = (y_pred_labels == y_test).sum().item()\n",
    "    accuracy = correct / len(y_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7a29a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Convert to CPU lists\n",
    "y_true = y_test.cpu().tolist()\n",
    "y_pred = y_pred_labels.cpu().tolist()\n",
    "\n",
    "# Compute matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Not L\", \"L\"], yticklabels=[\"Not L\", \"L\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f9dc4f",
   "metadata": {},
   "source": [
    "## Multi Shape Net\n",
    "Now, we create a simple nueral net that can distinguish different types of shapes in 3x3 grayscale images.\n",
    "\n",
    "Shapes: \"L\", \"box\", \"zigzag\", \"T\", \"column\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3ee858",
   "metadata": {},
   "source": [
    "### ðŸ“ Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912912f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAPE_CLASSES = [\"L\", \"Box\", \"Zigzag\", \"T\", \"Column\"]\n",
    "idx_from_label = {name: i for i, name in enumerate(SHAPE_CLASSES)}\n",
    "label_from_idx = {i: name for i, name in enumerate(SHAPE_CLASSES)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b8d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Sample images per class (grayscale binary, 3x3)\n",
    "samples = [\n",
    "    # L shapes\n",
    "    (torch.tensor([[1,0,0,0],[1,0,0,0],[1,1,0,0],[0,0,0,0]]), \"L\"),\n",
    "    (torch.tensor([[0,1,0,0],[0,1,0,0],[0,1,1,0],[0,0,0,0]]), \"L\"),\n",
    "    (torch.tensor([[0,0,0,0],[1,1,1,0],[0,0,1,0],[0,0,0,0]]), \"L\"),\n",
    "    (torch.tensor([[0,0,0,0],[0,0,0,0],[1,0,0,0],[1,1,1,0]]), \"L\"),\n",
    "\n",
    "    # Boxes (2x2)\n",
    "    (torch.tensor([[1,1,0,0],[1,1,0,0],[0,0,0,0],[0,0,0,0]]), \"Box\"),\n",
    "    (torch.tensor([[0,0,0,0],[1,1,0,0],[1,1,0,0],[0,0,0,0]]), \"Box\"),\n",
    "    (torch.tensor([[0,0,0,0], [0,0,0,0],[0,1,1,0],[0,1,1,0]]), \"Box\"),\n",
    "    (torch.tensor([[0,0,0,0], [0,1,1,0],[0,1,1,0],[0,0,0,0]]), \"Box\"),\n",
    "\n",
    "    # Zigzags\n",
    "    (torch.tensor([[1,0,0,0],[1,1,0,0],[0,1,0,0],[0,0,0,0]]), \"Zigzag\"),\n",
    "    (torch.tensor([[0,0,0,0],[0,1,1,0],[1,1,0,0],[0,0,0,0]]), \"Zigzag\"),\n",
    "    (torch.tensor([[0,0,0,0],[1,1,0,0],[0,1,1,0],[0,0,0,0]]), \"Zigzag\"),\n",
    "    (torch.tensor([[0,0,0,0],[0,1,0,0],[1,1,0,0],[1,0,0,0]]), \"Zigzag\"),\n",
    "\n",
    "    # Ts\n",
    "    (torch.tensor([[1,0,0,0],[1,1,0,0],[1,0,0,0],[0,0,0,0]]), \"T\"),\n",
    "    (torch.tensor([[0,0,0,0],[1,1,1,0],[0,1,0,0],[0,0,0,0]]), \"T\"),\n",
    "    (torch.tensor([[0,0,0,0],[1,1,1,0],[0,1,0,0],[0,0,0,0]]), \"T\"),\n",
    "    (torch.tensor([[0,0,0,0],[0,0,0,0],[0,0,1,0],[0,1,1,1]]), \"T\"),\n",
    "\n",
    "    # Columns\n",
    "    (torch.tensor([[0,1,0,0],[0,1,0,0],[0,1,0,0],[0,0,0,0]]), \"Column\"),\n",
    "    (torch.tensor([[1,0,0,0],[1,0,0,0],[1,0,0,0],[0,0,0,0]]), \"Column\"),\n",
    "    (torch.tensor([[0,0,0,0],[0,0,0,0],[0,0,0,0],[1,1,1,0]]), \"Column\"),\n",
    "    (torch.tensor([[0,0,0,0],[0,0,0,0],[1,1,1,0], [0,0,0,0]]), \"Column\"),\n",
    "]\n",
    "\n",
    "def augment_rotations(img: torch.Tensor):\n",
    "    \"\"\"Returns 4 rotations (0Â°, 90Â°, 180Â°, 270Â°) of a 3x3 image tensor.\"\"\"\n",
    "    return [img.rot90(k, dims=[0, 1]) for k in range(4)]\n",
    "\n",
    "\n",
    "def augment_with_flips_and_rotations(img):\n",
    "    rots = augment_rotations(img)\n",
    "    flips = [torch.flip(img, [0]), torch.flip(img, [1])]  # vertical, horizontal\n",
    "    return rots + flips\n",
    "\n",
    "\n",
    "augmented_X = []\n",
    "augmented_y = []\n",
    "\n",
    "for img, label in samples:\n",
    "    for aug in augment_with_flips_and_rotations(img):\n",
    "        augmented_X.append(aug.float())\n",
    "        augmented_y.append(idx_from_label[label])\n",
    "\n",
    "X_train = torch.stack(augmented_X).unsqueeze(1)  # Add channel dimension (1 for grayscale) -> (B, 1, H, W)\n",
    "y_train = torch.tensor(augmented_y)  # Shape (B,)\n",
    "print(f\"X_train shape: {X_train.shape}\\ny_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fef04e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize = True\n",
    "if visualize:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    cols = 12\n",
    "    fig, axes = plt.subplots(len(augmented_X) // cols, cols, figsize=(12, 9))\n",
    "    for i, (img, label) in enumerate(zip(augmented_X, augmented_y)):\n",
    "        ax = axes[i // cols, i % cols]\n",
    "        ax.imshow(img.numpy(), cmap='Greens', vmin=0, vmax=1)\n",
    "        ax.set_title(label_from_idx[label], fontsize=8)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eca311",
   "metadata": {},
   "source": [
    "### ðŸ§  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f833fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiShapeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiShapeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3)   # 8 x [1, 3, 3] filters\n",
    "        self.bn1 = nn.BatchNorm2d(8)                  # normalize all data by channel\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)      # downsample spatially\n",
    "\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=1)  # 16 x [8, 1, 1] filters\n",
    "        self.bn2 = nn.BatchNorm2d(16)                 # normalize all data by channel\n",
    "        self.dropout = nn.Dropout(0.2)               # randomly zero out features\n",
    "\n",
    "        self.fc = nn.Linear(16 * 1 * 1, 5)            # output shape after pool = [B, 16, 1, 1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))   # [B, 8, 2, 2]\n",
    "        x = self.pool1(x)                     # [B, 8, 1, 1]\n",
    "        x = F.relu(self.bn2(self.conv2(x)))   # [B, 16, 1, 1]\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)             # [B, 16]\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e20f636",
   "metadata": {},
   "source": [
    "### âš™ï¸ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bc3e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiShapeNet()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(250):\n",
    "    out = model(X_train)\n",
    "    loss = loss_fn(out, y_train)\n",
    "    losses.append(loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cf487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_filters(layer: nn.Conv2d, title=\"Conv Filters\"):\n",
    "    with torch.no_grad():\n",
    "        filters = layer.weight.cpu().detach().clone()\n",
    "    \n",
    "    num_filters = filters.shape[0]  # out_channels\n",
    "    fig, _ = plt.subplots(1, num_filters, figsize=(num_filters * 2, 2))\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    for i, subplot in enumerate(fig.axes):\n",
    "        filter_img = filters[i][0]  # Assuming in_channels=1\n",
    "        subplot.imshow(filter_img, cmap='viridis')\n",
    "        subplot.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if visualize:\n",
    "    plt.plot(losses, label='Loss')\n",
    "    min_loss_value = min(losses)\n",
    "    min_loss_epoch = losses.index(min_loss_value)\n",
    "    plt.scatter(min_loss_epoch, min_loss_value, color='red', label=f'Min Loss: {min_loss_value:.4f}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss with Min Loss Highlighted')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    visualize_filters(model.conv1, title=\"Conv1 Filters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd131a53",
   "metadata": {},
   "source": [
    "### ðŸ§ª Create Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad3cf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = [\n",
    "    # True label: \"L\"\n",
    "    (torch.tensor([[1,0,0,0],[1,0,0,0],[1,1,0,0],[0,0,0,0]]), \"L\"),\n",
    "    (torch.tensor([[0,0,0,0],[1,0,0,0],[1,1,1,0],[0,0,0,0]]), \"L\"),\n",
    "    (torch.tensor([[0,0,1,0],[1,1,1,0],[0,0,0,0],[0,0,0,0]]), \"L\"),\n",
    "    # True label: \"Box\"\n",
    "    (torch.tensor([[0,1,1,0],[0,1,1,0],[0,0,0,0],[0,0,0,0]]), \"Box\"),\n",
    "    (torch.tensor([[0,0,0,0],[0,1,1,0],[0,1,1,0],[0,0,0,0]]), \"Box\"),\n",
    "    (torch.tensor([[0,0,0,0],[1,1,0,0],[1,1,0,0],[0,0,0,0]]), \"Box\"),\n",
    "    # True label: \"Zigzag\"\n",
    "    (torch.tensor([[1,1,0,0],[0,1,1,0],[0,0,0,0],[0,0,0,0]]), \"Zigzag\"),\n",
    "    (torch.tensor([[0,0,0,0],[0,1,1,0],[1,1,0,0],[0,0,0,0]]), \"Zigzag\"),\n",
    "    (torch.tensor([[0,1,0,0],[0,1,1,0],[0,0,1,0],[0,0,0,0]]), \"Zigzag\"),\n",
    "    # True label: \"T\"\n",
    "    (torch.tensor([[0,0,1,0],[0,1,1,0],[0,0,1,0],[0,0,0,0]]), \"T\"),\n",
    "    (torch.tensor([[0,0,0,0],[0,1,0,0],[1,1,1,0],[0,0,0,0]]), \"T\"),\n",
    "    (torch.tensor([[0,0,0,0],[1,1,1,0],[0,1,0,0],[0,0,0,0]]), \"T\"),\n",
    "    # True label: \"Column\"\n",
    "    (torch.tensor([[0,1,0,0],[0,1,0,0],[0,1,0,0],[0,0,0,0]]), \"Column\"),\n",
    "    (torch.tensor([[1,0,0,0],[1,0,0,0],[1,0,0,0],[0,0,0,0]]), \"Column\"),\n",
    "    (torch.tensor([[0,0,0,0],[0,0,0,0],[1,1,1,0],[0,0,0,0]]), \"Column\"),\n",
    "]\n",
    "\n",
    "X_test = torch.stack([img.float().unsqueeze(0) for img, _ in test_samples])  # Add channel dimension (1 for grayscale) -> (B, 1, H, W)\n",
    "y_test = torch.tensor([[idx_from_label[label]] for _, label in test_samples]) # Convert labels to indices -> (B, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68653290",
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualize:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    num_images = len(X_test)\n",
    "    num_cols = 5\n",
    "    num_rows = (num_images + num_cols - 1) // num_cols  # Calculate rows dynamically\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(8, 3))\n",
    "\n",
    "    for i, (img, label) in enumerate(zip(X_test, y_test)):\n",
    "        ax = axes[i // num_cols, i % num_cols]\n",
    "        ax.imshow(img.numpy().squeeze(), cmap='Greens', vmin=0, vmax=1)\n",
    "        ax.set_title(label_from_idx[int(label.item())])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559e0387",
   "metadata": {},
   "source": [
    "### âœ… Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1083ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(X_test)\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    print(\"Test predictions:\", [label_from_idx[int(idx.item())] for idx in preds])\n",
    "    print(\"True labels     :\", [label_from_idx[int(idx.item())] for idx in y_test.squeeze()])\n",
    "    correct = (preds == y_test.squeeze()).sum().item()\n",
    "    accuracy = correct / len(y_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38412cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test.tolist(), preds.tolist())\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=SHAPE_CLASSES, yticklabels=SHAPE_CLASSES)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2000bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Input: 2 images (batch), 3 channels (filters), 2x2 spatial\n",
    "x = torch.tensor([\n",
    "    [  # Image 1\n",
    "        [[1.0, 2.0], [3.0, 4.0]],  # Channel 1\n",
    "        [[5.0, 6.0], [7.0, 8.0]],  # Channel 2\n",
    "        [[9.0, 10.0], [11.0, 12.0]]  # Channel 3\n",
    "    ],\n",
    "    [  # Image 2\n",
    "        [[2.0, 3.0], [4.0, 5.0]],\n",
    "        [[6.0, 7.0], [8.0, 9.0]],\n",
    "        [[10.0, 11.0], [12.0, 13.0]]\n",
    "    ]\n",
    "])  # shape = (2, 3, 2, 2)\n",
    "\n",
    "bn = nn.BatchNorm2d(3)  # One set of stats per channel\n",
    "\n",
    "# In eval mode, it uses running stats. In training mode, it uses current batch.\n",
    "bn.train()\n",
    "normalized = bn(x)\n",
    "\n",
    "print(\"Output after BatchNorm2d:\\n\", normalized)\n",
    "print(\"Shape of normalized output:\", normalized.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
